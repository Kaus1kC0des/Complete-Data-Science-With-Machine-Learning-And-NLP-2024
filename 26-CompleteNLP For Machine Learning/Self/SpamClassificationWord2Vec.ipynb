{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:24:02.299195Z",
     "start_time": "2025-04-04T08:24:01.358913Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "id": "9c5953e2077ba252",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:24:02.316597Z",
     "start_time": "2025-04-04T08:24:02.306974Z"
    }
   },
   "source": [
    "df = pd.read_csv(\"SpamClassifier-master/smsspamcollection/SMSSpamCollection\", sep=\"\\t\", names=[\"label\", \"message\"])"
   ],
   "id": "765c157b41962bbb",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:24:02.369543Z",
     "start_time": "2025-04-04T08:24:02.364698Z"
    }
   },
   "source": [
    "df.head()"
   ],
   "id": "18f9219706da04e1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:24:02.432532Z",
     "start_time": "2025-04-04T08:24:02.428217Z"
    }
   },
   "source": [
    "from tqdm import tqdm\n",
    "class SpamWord2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SpamWord2Vec, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optimizer = optim.AdamW(self.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = self.linear(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def to_empty(self, *, device, recurse = True):\n",
    "        return super().to_empty(device=device, recurse=recurse)\n",
    "    \n",
    "    def train_model(self, train_loader, test_loader, num_epochs=5):\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        train_accuracies = []\n",
    "        test_accuracies = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            train_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "                self.optimizer.zero_grad()\n",
    "                inputs, labels = batch\n",
    "                outputs = self(inputs)\n",
    "                outputs = outputs.reshape(-1)\n",
    "                loss = self.criterion(outputs, labels.float())\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "            train_losses.append(train_loss / len(train_loader))\n",
    "            train_accuracies.append(correct / total)\n",
    "\n",
    "            # Evaluate on test set\n",
    "            self.eval()\n",
    "            test_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in test_loader:\n",
    "                    inputs, labels = batch\n",
    "                    outputs = self(inputs)\n",
    "                    outputs = outputs.reshape(-1)\n",
    "                    loss = self.criterion(outputs, labels.float())\n",
    "                    test_loss += loss.item()\n",
    "                    predicted = (outputs > 0.5).float()\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    total += labels.size(0)\n",
    "\n",
    "            test_losses.append(test_loss / len(test_loader))\n",
    "            test_accuracies.append(correct / total)\n",
    "            tqdm.write(f\"Train Loss: {train_losses[-1]:.4f}, Train Accuracy: {train_accuracies[-1]:.4f}\")\n",
    "            tqdm.write(f\"Test Loss: {test_losses[-1]:.4f}, Test Accuracy: {test_accuracies[-1]:.4f}\")\n",
    "        # return train_losses, test_losses, train_accuracies, test_accuracies"
   ],
   "id": "62ac1ae36a06e629",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:24:08.624275Z",
     "start_time": "2025-04-04T08:24:02.472280Z"
    }
   },
   "source": [
    "import nltk\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.texts = df['message'].values\n",
    "        self.labels = df['label'].map({'ham': 0, 'spam': 1}).values\n",
    "        self.vocab = set()\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {}\n",
    "        self.processed_texts = []  # Store preprocessed texts as token indices\n",
    "        self.build_vocab()\n",
    "\n",
    "    @classmethod\n",
    "    def preprocess_text(cls, text):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        text = text.lower()\n",
    "        tokens = word_tokenize(text)  # Use word_tokenize for tokenization\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in stop_words]\n",
    "        return tokens\n",
    "\n",
    "    def text_to_indices(self, tokens):\n",
    "        indices = [self.word_to_index[token] for token in tokens if token in self.word_to_index]\n",
    "        return indices\n",
    "\n",
    "    def build_vocab(self):\n",
    "        for text in self.texts:\n",
    "            tokens = self.preprocess_text(text)\n",
    "            self.processed_texts.append(tokens)  # Store preprocessed tokens\n",
    "            for token in tokens:\n",
    "                self.vocab.add(token)\n",
    "        self.word_to_index = {word: i + 1 for i, word in enumerate(self.vocab)}  # Start indexing from 1\n",
    "        self.index_to_word = {i + 1: word for i, word in enumerate(self.vocab)}\n",
    "        self.vocab_size = len(self.word_to_index) + 1  # +1 for padding index\n",
    "        self.texts = [self.text_to_indices(tokens) for tokens in self.processed_texts]  # Convert texts to indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = torch.tensor(self.texts[idx], dtype=torch.long, device=self.device)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long, device=self.device)  # Use long for classification\n",
    "        return text, label\n",
    "\n",
    "    def predict(self, text):\n",
    "        tokens = self.preprocess_text(text)\n",
    "        indices = self.text_to_indices(tokens)\n",
    "        text_tensor = torch.tensor(indices, dtype=torch.long, device=self.device)\n",
    "        return text_tensor"
   ],
   "id": "d602bfd5d338052a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/kausik/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/kausik/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/kausik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:24:10.589080Z",
     "start_time": "2025-04-04T08:24:08.639981Z"
    }
   },
   "source": [
    "dataset = SpamDataset(df)"
   ],
   "id": "22fb8b69bd25a83",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:24:10.744515Z",
     "start_time": "2025-04-04T08:24:10.592777Z"
    }
   },
   "source": [
    "train_df, test_df = train_test_split(dataset, test_size=0.2, random_state=42, shuffle=True)"
   ],
   "id": "742d0ccc9c18e6ee",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:24:10.754173Z",
     "start_time": "2025-04-04T08:24:10.752517Z"
    }
   },
   "source": [
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    texts = nn.utils.rnn.pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "    labels = torch.stack(labels)\n",
    "    return texts, labels"
   ],
   "id": "2581ecc12e14440b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:24:10.798558Z",
     "start_time": "2025-04-04T08:24:10.795470Z"
    }
   },
   "source": [
    "train_loader = DataLoader(train_df, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_df, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ],
   "id": "de8b56c905a2992a",
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:24:11.114599Z",
     "start_time": "2025-04-04T08:24:10.841475Z"
    }
   },
   "source": "model = SpamWord2Vec(vocab_size=dataset.vocab_size, embedding_dim=100)",
   "id": "b2527033e5c612ed",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:24:11.125004Z",
     "start_time": "2025-04-04T08:24:11.122430Z"
    }
   },
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ],
   "id": "1b2ed7d5302afadd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpamWord2Vec(\n",
       "  (embeddings): Embedding(7603, 100)\n",
       "  (linear): Linear(in_features=100, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (criterion): BCELoss()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:24:36.198895Z",
     "start_time": "2025-04-04T08:24:34.150591Z"
    }
   },
   "source": "model.train_model(train_loader, test_loader, num_epochs=15)",
   "id": "df13f0b000644c54",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 140/140 [00:00<00:00, 537.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3258, Train Accuracy: 0.8670\n",
      "Test Loss: 0.3096, Test Accuracy: 0.8726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|██████████| 140/140 [00:00<00:00, 1127.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2905, Train Accuracy: 0.8838\n",
      "Test Loss: 0.2805, Test Accuracy: 0.8807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|██████████| 140/140 [00:00<00:00, 1248.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2445, Train Accuracy: 0.9037\n",
      "Test Loss: 0.2360, Test Accuracy: 0.8969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|██████████| 140/140 [00:00<00:00, 1236.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2027, Train Accuracy: 0.9219\n",
      "Test Loss: 0.1898, Test Accuracy: 0.9157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|██████████| 140/140 [00:00<00:00, 1186.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1538, Train Accuracy: 0.9468\n",
      "Test Loss: 0.1533, Test Accuracy: 0.9381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|██████████| 140/140 [00:00<00:00, 1195.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1212, Train Accuracy: 0.9583\n",
      "Test Loss: 0.1277, Test Accuracy: 0.9570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|██████████| 140/140 [00:00<00:00, 1197.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1027, Train Accuracy: 0.9686\n",
      "Test Loss: 0.1103, Test Accuracy: 0.9686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|██████████| 140/140 [00:00<00:00, 1203.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0834, Train Accuracy: 0.9737\n",
      "Test Loss: 0.0980, Test Accuracy: 0.9722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|██████████| 140/140 [00:00<00:00, 1214.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0773, Train Accuracy: 0.9735\n",
      "Test Loss: 0.0892, Test Accuracy: 0.9722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|██████████| 140/140 [00:00<00:00, 1217.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0687, Train Accuracy: 0.9771\n",
      "Test Loss: 0.0826, Test Accuracy: 0.9731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|██████████| 140/140 [00:00<00:00, 1202.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0580, Train Accuracy: 0.9798\n",
      "Test Loss: 0.0773, Test Accuracy: 0.9749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|██████████| 140/140 [00:00<00:00, 1207.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0558, Train Accuracy: 0.9818\n",
      "Test Loss: 0.0728, Test Accuracy: 0.9767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|██████████| 140/140 [00:00<00:00, 1214.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0477, Train Accuracy: 0.9832\n",
      "Test Loss: 0.0695, Test Accuracy: 0.9776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|██████████| 140/140 [00:00<00:00, 1202.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0492, Train Accuracy: 0.9825\n",
      "Test Loss: 0.0678, Test Accuracy: 0.9785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15: 100%|██████████| 140/140 [00:00<00:00, 1235.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0404, Train Accuracy: 0.9877\n",
      "Test Loss: 0.0648, Test Accuracy: 0.9785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:27:41.888962Z",
     "start_time": "2025-04-04T08:27:41.863687Z"
    }
   },
   "source": "torch.save(model.state_dict(), \"SpamWord2Vec_StateDict.pth\")",
   "id": "96a9a945ad028c48",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:27:54.553493Z",
     "start_time": "2025-04-04T08:27:54.542367Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model, \"SpamWord2Vec_Model.pth\")",
   "id": "43df2eed5631423a",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CBOW Implementation",
   "id": "64d85e53dfe45ccc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:34:55.485549Z",
     "start_time": "2025-04-04T08:34:55.482713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # Added softmax layer\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).sum(dim=1)\n",
    "        out = self.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        out = self.softmax(out) # Apply softmax\n",
    "        return out"
   ],
   "id": "f63469f0a2d53b52",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:31:39.044639Z",
     "start_time": "2025-04-04T08:31:39.041567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def create_vocab(corpus):\n",
    "    vocab = set()\n",
    "    for sentence in corpus:\n",
    "        for word in sentence:\n",
    "            vocab.add(word)\n",
    "    word_to_index = {word: index for index, word in enumerate(vocab)}\n",
    "    index_to_word = {index: word for index, word in enumerate(vocab)}\n",
    "    return word_to_index, index_to_word\n",
    "\n",
    "def create_training_data(corpus, word_to_index, context_size):\n",
    "    data = []\n",
    "    for sentence in corpus:\n",
    "        for i in range(context_size, len(sentence) - context_size):\n",
    "            context_words = [sentence[j] for j in range(i - context_size, i + context_size + 1) if j != i]\n",
    "            target_word = sentence[i]\n",
    "            context_indices = [word_to_index[word] for word in context_words]\n",
    "            target_index = word_to_index[target_word]\n",
    "            data.append((context_indices, target_index))\n",
    "    return data"
   ],
   "id": "7f00b4e673f69fbe",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:31:52.486320Z",
     "start_time": "2025-04-04T08:31:52.484417Z"
    }
   },
   "cell_type": "code",
   "source": "corpus = df['message'].values",
   "id": "39892aaa40f18c0c",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:32:14.915373Z",
     "start_time": "2025-04-04T08:32:14.336876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocess the corpus\n",
    "corpus = [preprocess_text(sentence) for sentence in corpus]"
   ],
   "id": "8b93ce33512650b7",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:32:30.536741Z",
     "start_time": "2025-04-04T08:32:30.530657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create vocabulary\n",
    "word_to_index, index_to_word = create_vocab(corpus)\n",
    "vocab_size = len(word_to_index)"
   ],
   "id": "41c3613a6599d43b",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:32:50.454879Z",
     "start_time": "2025-04-04T08:32:50.317497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create training data\n",
    "context_size = 3\n",
    "training_data = create_training_data(corpus, word_to_index, context_size)"
   ],
   "id": "dc0eb9648cc755b7",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:33:17.008793Z",
     "start_time": "2025-04-04T08:33:16.988221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the model\n",
    "embedding_dim = 100\n",
    "model = CBOW(vocab_size, embedding_dim, context_size)\n",
    "model.to(\"cuda\")"
   ],
   "id": "3a4a4d7d1b6ad117",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBOW(\n",
       "  (embeddings): Embedding(8109, 100)\n",
       "  (linear1): Linear(in_features=100, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=8109, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:35:33.029388Z",
     "start_time": "2025-04-04T08:35:33.027344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define loss function and optimizer\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.00001)"
   ],
   "id": "12d49c3252010e04",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:37:35.840329Z",
     "start_time": "2025-04-04T08:37:35.771694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "num_epochs = 50\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    total_loss = 0\n",
    "    for context, target in training_data:\n",
    "        context_var = torch.tensor(context, dtype=torch.long, device=\"cuda\")\n",
    "        target_var = torch.tensor([target], dtype=torch.long, device=\"cuda\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model(context_var)\n",
    "        loss = loss_function(log_probs, target_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    tqdm.write(f\"Epoch: {epoch+1}, Loss: {total_loss:.4f}\")"
   ],
   "id": "cb6659fe20da3c46",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x6 and 100x128)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[27]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      7\u001B[39m target_var = torch.tensor([target], dtype=torch.long, device=\u001B[33m\"\u001B[39m\u001B[33mcuda\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      9\u001B[39m optimizer.zero_grad()\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m log_probs = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontext_var\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     11\u001B[39m loss = loss_function(log_probs, target_var)\n\u001B[32m     12\u001B[39m loss.backward()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/torch-gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1530\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1531\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1532\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/torch-gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1536\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1537\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1538\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1539\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1540\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1541\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1543\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1544\u001B[39m     result = \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 22\u001B[39m, in \u001B[36mCBOW.forward\u001B[39m\u001B[34m(self, inputs)\u001B[39m\n\u001B[32m     20\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs):\n\u001B[32m     21\u001B[39m     embeds = \u001B[38;5;28mself\u001B[39m.embeddings(inputs).sum(dim=\u001B[32m1\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m22\u001B[39m     out = \u001B[38;5;28mself\u001B[39m.relu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlinear1\u001B[49m\u001B[43m(\u001B[49m\u001B[43membeds\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m     23\u001B[39m     out = \u001B[38;5;28mself\u001B[39m.linear2(out)\n\u001B[32m     24\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/torch-gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1530\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1531\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1532\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/torch-gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1536\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1537\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1538\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1539\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1540\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1541\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1543\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1544\u001B[39m     result = \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/torch-gpu/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    115\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mRuntimeError\u001B[39m: mat1 and mat2 shapes cannot be multiplied (1x6 and 100x128)"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:42:58.836217Z",
     "start_time": "2025-04-04T08:42:58.830397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# python\n",
    "import nltk\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, df, context_size):\n",
    "        \"\"\"\n",
    "        Expects a pandas DataFrame with a column \"message\".\n",
    "        The dataset will preprocess messages and build vocabulary.\n",
    "        \"\"\"\n",
    "        self.context_size = context_size  # number of words on left/right\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.messages = df['message'].values\n",
    "        self.processed_texts = [self.preprocess_text(text) for text in self.messages]\n",
    "        self.build_vocab()\n",
    "        self.data = self.create_training_data()\n",
    "\n",
    "    @classmethod\n",
    "    def preprocess_text(cls, text):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        text = text.lower()\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in stop_words]\n",
    "        return tokens\n",
    "\n",
    "    def build_vocab(self):\n",
    "        vocab = set()\n",
    "        for tokens in self.processed_texts:\n",
    "            vocab.update(tokens)\n",
    "        # index 0 is reserved for padding; start vocab indexing at 1\n",
    "        self.word2idx = {word: idx+1 for idx, word in enumerate(sorted(vocab))}\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "        self.vocab_size = len(self.word2idx) + 1\n",
    "\n",
    "    def create_training_data(self):\n",
    "        \"\"\"\n",
    "        Creates training pairs (context, target). For each word in a sentence,\n",
    "        the context is defined as the surrounding 2*context_size words.\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        # require at least 2*context_size+1 tokens per sentence\n",
    "        for tokens in self.processed_texts:\n",
    "            if len(tokens) < 2 * self.context_size + 1:\n",
    "                continue\n",
    "            for i in range(self.context_size, len(tokens) - self.context_size):\n",
    "                # context is all words except the target\n",
    "                context_words = tokens[i - self.context_size:i] + tokens[i + 1:i + self.context_size + 1]\n",
    "                target_word = tokens[i]\n",
    "                # convert tokens to indices (ignore words not in vocab)\n",
    "                context_indices = [self.word2idx[w] for w in context_words if w in self.word2idx]\n",
    "                if len(context_indices) != 2 * self.context_size:\n",
    "                    continue\n",
    "                target_index = self.word2idx.get(target_word)\n",
    "                if target_index is not None:\n",
    "                    data.append((context_indices, target_index))\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.data[idx]\n",
    "        # return as tensors on the proper device\n",
    "        context_tensor = torch.tensor(context, dtype=torch.long, device=self.device)\n",
    "        target_tensor = torch.tensor(target, dtype=torch.long, device=self.device)\n",
    "        return context_tensor, target_tensor\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.word2idx, self.idx2word\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size"
   ],
   "id": "252bfea85f6e0e05",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/kausik/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/kausik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/kausik/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T08:43:14.618747Z",
     "start_time": "2025-04-04T08:43:14.611716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim=128):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        # these must be set externally after creating dataset\n",
    "        self.word2idx = None\n",
    "        self.idx2word = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs: shape [batch, 2*context_size]\n",
    "        embeds = self.embeddings(inputs)\n",
    "        # sum embeddings to form a context representation [batch, embedding_dim]\n",
    "        context_vector = torch.sum(embeds, dim=1)\n",
    "        out = self.relu(self.linear1(context_vector))\n",
    "        out = self.linear2(out)\n",
    "        out = self.log_softmax(out)\n",
    "        return out\n",
    "\n",
    "    def predict(self, context_tokens):\n",
    "        \"\"\"\n",
    "        Given a list of context tokens (length = 2*context_size),\n",
    "        returns the target word predicted by the model.\n",
    "        Ensure that self.word2idx and self.idx2word are set.\n",
    "        \"\"\"\n",
    "        if not self.word2idx or not self.idx2word:\n",
    "            raise ValueError(\"Model vocabulary not set. Assign word2idx and idx2word.\")\n",
    "        indices = []\n",
    "        for token in context_tokens:\n",
    "            idx = self.word2idx.get(token)\n",
    "            if idx is None:\n",
    "                raise ValueError(f'Token \\{token\\} not in vocabulary.')\n",
    "            indices.append(idx)\n",
    "        # Convert to tensor and add batch dimension\n",
    "        context_tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0)\n",
    "        context_tensor = context_tensor.to(self.embeddings.weight.device)\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            out = self.forward(context_tensor)\n",
    "            predicted_idx = torch.argmax(out, dim=1).item()\n",
    "        return self.idx2word.get(predicted_idx, \"<UNK>\")"
   ],
   "id": "d8192ac077e8dbca",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string expression part cannot include a backslash (3363294344.py, line 40)",
     "output_type": "error",
     "traceback": [
      "  \u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[29]\u001B[39m\u001B[32m, line 40\u001B[39m\n\u001B[31m    \u001B[39m\u001B[31mraise ValueError(f'Token \\{token\\} not in vocabulary.')\u001B[39m\n                                                          ^\n\u001B[31mSyntaxError\u001B[39m\u001B[31m:\u001B[39m f-string expression part cannot include a backslash\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "268a44f909510cb0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
